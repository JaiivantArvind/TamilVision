# TamilVision 156

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white"/>
  <img src="https://img.shields.io/badge/PyTorch-2.1.0-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white"/>
  <img src="https://img.shields.io/badge/FastAPI-0.104-009688?style=for-the-badge&logo=fastapi&logoColor=white"/>
  <img src="https://img.shields.io/badge/OpenCV-4.8-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white"/>
  <img src="https://img.shields.io/badge/Next.js-16-000000?style=for-the-badge&logo=nextdotjs&logoColor=white"/>
  <img src="https://img.shields.io/badge/Three.js-0.183-black?style=for-the-badge&logo=threedotjs&logoColor=white"/>
  <img src="https://img.shields.io/badge/CUDA-11.8-76B900?style=for-the-badge&logo=nvidia&logoColor=white"/>
</p>

> **Real-time Tamil handwritten character recognition in the browser.**  
> Draw a character on the canvas, upload a scan, or drop a photo â€” TamilVision identifies it from **156 Unicode Tamil characters** in under 50 ms.

---

## âœ¨ Features

| Feature | Detail |
|---|---|
| ğŸ–Šï¸ **Canvas drawing** | Draw directly in the browser with an adjustable-width brush on a touch & mouse-friendly canvas |
| ğŸ“ **Image upload** | Drag-and-drop or browse for PNG, JPG, or BMP â€” including transparent PNGs and scanned documents |
| âš¡ **Real-time prediction** | Top-3 predictions returned with confidence scores and animated progress bars |
| ğŸ”€ **Universal preprocessing** | Handles white-on-black canvas art *and* black-on-white scans through the same robust OpenCV pipeline |
| ğŸ§  **156-class coverage** | Vowels (à®‰à®¯à®¿à®°à¯), pure consonants (à®®à¯†à®¯à¯), base consonants, and all six vowel-marker series |
| ğŸ“Š **Confidence colouring** | Green â‰¥ 70 %, amber 40â€“70 %, red < 40 % â€” instant visual feedback on prediction quality |
| ğŸŒ„ **Animated background** | GLSL Perlin-noise hills rendered in real-time with Three.js WebGL â€” zero impact on prediction latency |
| ğŸªŸ **Glassmorphism UI** | Panels use `backdrop-blur` + translucent fills so the WebGL background shows through; sky-blue accent throughout |

---

## ğŸ—‚ï¸ Project Structure

```
TamilVision/
â”œâ”€â”€ frontend/                   # React / Next.js 16 frontend (shadcn + Tailwind v4)
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ layout.tsx          # Root layout â€” dark theme, Mukta Malar font
â”‚   â”‚   â”œâ”€â”€ page.tsx            # Full TamilVision UI (canvas, upload, results)
â”‚   â”‚   â””â”€â”€ globals.css         # Tailwind v4 + shadcn CSS variables
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ ui/
â”‚   â”‚       â””â”€â”€ glsl-hills.tsx  # Three.js GLSL Perlin-noise hills background
â”‚   â”œâ”€â”€ lib/utils.ts            # shadcn utility helpers
â”‚   â”œâ”€â”€ package.json            # React 19, Next.js 16, Three.js 0.183
â”‚   â””â”€â”€ index_vanilla.html      # Original vanilla JS/HTML backup
â”‚
â”œâ”€â”€ app/
â”‚   â””â”€â”€ main.py                 # FastAPI server & /predict endpoint
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py               # 156 Tamil class labels & hyperparameters
â”‚   â”œâ”€â”€ model.py                # TamilVision architecture (MobileNetV3-Small)
â”‚   â”œâ”€â”€ preprocess.py           # OpenCV inference pipeline + torchvision train transforms
â”‚   â”œâ”€â”€ dataset.py              # PyTorch Dataset with shared-memory RAM cache
â”‚   â””â”€â”€ train.py                # Full training loop (AMP, AdamW, cosine LR)
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ best_model.pth          # Trained checkpoint (~19 MB)
â”‚
â”œâ”€â”€ scripts/                    # Utility scripts (visualize, validate, sanity-check, auto-tune)
â”œâ”€â”€ data/                       # Dataset root (gitignored â€” see Dataset section)
â”œâ”€â”€ requirements.txt            # Python backend dependencies
â””â”€â”€ .gitignore
```

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|---|---|
| **Model** | PyTorch 2.1 Â· MobileNetV3-Small Â· ImageNet pretrained weights |
| **Preprocessing** | OpenCV 4.8 Â· NumPy Â· Pillow |
| **Backend** | FastAPI 0.104 Â· Uvicorn Â· python-multipart |
| **Frontend** | Next.js 16 Â· React 19 Â· TypeScript Â· Tailwind CSS v4 Â· shadcn/ui |
| **3-D Background** | Three.js 0.183 Â· GLSL Perlin-noise vertex shader (`GLSLHills`) |
| **Training** | Mixed-precision AMP Â· AdamW Â· CosineAnnealingLR Â· Label smoothing |
| **Hardware** | NVIDIA GTX 1650 Â· CUDA 11.8 |

---

## ğŸŒ„ GLSLHills â€” Animated Background

The animated wireframe hills are a self-contained React component (`components/ui/glsl-hills.tsx`) that runs entirely on the GPU via Three.js + custom GLSL shaders. It is zero-dependency beyond Three.js and adds **no overhead** to the prediction pipeline.

### How it works

| Step | Detail |
|---|---|
| **Geometry** | `PlaneGeometry(256, 256, 256, 256)` â€” 256Ã—256 subdivided flat plane |
| **Vertex shader** | Rotates the plane to face the camera, then displaces each vertex vertically using **3-octave Classic Perlin Noise** (`cnoise`). The noise input drifts along the Z axis over `time`, creating the flowing hills illusion. |
| **Fragment shader** | Solid grey (`vec3(0.6)`) with opacity that fades out with distance â€” edges dissolve naturally. |
| **Animation** | A `requestAnimationFrame` loop advances `uniforms.time` each frame. The loop is cancelled on React unmount to prevent memory leaks. |
| **Resize** | A `window.resize` listener keeps the camera aspect ratio and renderer size in sync. |

### Props

| Prop | Type | Default | Description |
|---|---|---|---|
| `width` | `string` | `"100vw"` | Container width |
| `height` | `string` | `"100vh"` | Container height |
| `cameraZ` | `number` | `125` | Camera Z distance (zoom) |
| `planeSize` | `number` | `256` | Plane subdivisions & size |
| `speed` | `number` | `0.5` | Animation speed multiplier |

---

### 1 â€” Model Architecture

`TamilVision` is a fine-tuned **MobileNetV3-Small** with two surgical modifications:

1. **Grayscale input adapter** â€” the first 3-channel RGB conv layer is replaced with a single-channel layer. The pretrained RGB weights are *summed* across the channel axis, preserving all learned edge and texture detectors without discarding any prior knowledge.
2. **New classifier head** â€” the final `Linear(1024 â†’ 1000)` ImageNet head is replaced with `Linear(1024 â†’ 156)`, initialised with Kaiming-uniform weights.

The model accepts tensors of shape `[B, 1, 128, 128]` normalised to `[-1, 1]` and outputs `[B, 156]` logits.

---

### 2 â€” OpenCV Inference Preprocessing Pipeline

The biggest challenge this project solves is the **domain gap**: the training data is *black ink on white*, but the browser canvas produces *white ink on black* with large empty margins. The 12-step pipeline in `src/preprocess.py` bridges that gap for every possible input type:

```
Canvas PNG             Uploaded JPG/PNG        Transparent PNG
(white on black)       (black on white)        (BGRA)
       â”‚                      â”‚                     â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Step 1 Â· cv2.IMREAD_UNCHANGED           â”‚
          â”‚  4-channel BGRA â†’ alpha-composite onto   â”‚
          â”‚  solid white â†’ convert to grayscale      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Step 2 Â· Otsu's Binary Threshold        â”‚
          â”‚  Forces every pixel to pure 0 or 255,   â”‚
          â”‚  eliminating JPEG compression noise      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Step 3 Â· Smart Background Inversion     â”‚
          â”‚  np.mean(img) > 127 â†’ light background  â”‚
          â”‚  detected â†’ cv2.bitwise_not to flip      â”‚
          â”‚  Result: white character on black bg     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Step 4 Â· Median Blur (3Ã—3)              â”‚
          â”‚  Removes single-pixel JPG artefacts      â”‚
          â”‚  without blurring stroke edges           â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Step 5 Â· Bounding-Box Crop              â”‚
          â”‚  cv2.findNonZero + cv2.boundingRect      â”‚
          â”‚  Discards all surrounding black margin   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Step 6 Â· Aspect-Ratio Square Pad        â”‚
          â”‚  Pads the shorter axis with black pixels â”‚
          â”‚  â†’ perfect square, no stretching         â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Step 7 Â· Uniform Border Padding (15 px) â”‚
          â”‚  cv2.copyMakeBorder, value=0             â”‚
          â”‚  Glyph never touches the image edge      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Step 8 Â· Final Invert                   â”‚
          â”‚  cv2.bitwise_not â†’ black ink on white    â”‚
          â”‚  Matches training data format exactly    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Step 9 Â· Gaussian Blur (3Ã—3, Ïƒ=1.5)    â”‚
          â”‚  Softens hard digital edges to match     â”‚
          â”‚  the scanned-ink texture of the dataset  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Step 10 Â· Resize to 128Ã—128 (Lanczos)  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Step 11 Â· Normalise â†’ [-1, 1]           â”‚
          â”‚  tensor = (pixel/255 âˆ’ 0.5) / 0.5        â”‚
          â”‚  Shape: [1, 1, 128, 128] float32         â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
                       TamilVision Model
                              â–¼
                    Softmax â†’ Top-3 results
```

---

### 3 â€” Training Details

| Setting | Value |
|---|---|
| Dataset | uTHCD (80/20 split) |
| Classes | 156 Tamil Unicode characters |
| Input size | 128 Ã— 128 grayscale |
| Batch size | 256 |
| Epochs | 30 |
| Optimiser | AdamW (lr=1e-3, weight_decay=1e-4) |
| Scheduler | 3-epoch linear warm-up â†’ CosineAnnealingLR |
| Loss | CrossEntropyLoss with label smoothing = 0.1 |
| Augmentation | RandomRotation Â±20Â°, RandomAffine, RandomPerspective, ElasticTransform |
| Precision | Mixed (AMP / FP16) |

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10 or higher
- Node.js 18+ and npm (for the React frontend)
- NVIDIA GPU with CUDA 11.8 recommended (CPU also works â€” see note below)
- Git

### 1 â€” Clone the repository

```bash
git clone https://github.com/JaiivantArvind/TamilVision.git
cd TamilVision
```

### 2 â€” Create and activate a virtual environment

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# macOS / Linux
python -m venv venv
source venv/bin/activate
```

### 3 â€” Install dependencies

```bash
pip install -r requirements.txt
```

> **CPU-only machines:** Edit the first two lines of `requirements.txt` before installing:
> ```
> --index-url https://download.pytorch.org/whl/cpu
> torch==2.1.0
> torchvision==0.16.0
> ```

### 4 â€” Start the API server

```bash
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

Expected output:
```
[TamilVision] Model loaded â€” device: cuda | best val acc: XX.XX%
INFO:     Uvicorn running on http://0.0.0.0:8000
```

### 5 â€” Start the React frontend

Open a **second terminal** tab and run:

```bash
cd frontend
npm install        # first run only â€” installs Next.js, Three.js, shadcn, etc.
npm run dev
```

Then open [http://localhost:3000](http://localhost:3000) in your browser.

The animated GLSL wireframe hills render in the background while the prediction panels load on top.  
The status dot in the top-right corner turns **green** when the frontend is connected to the FastAPI server.

> **Vanilla fallback:** `frontend/index_vanilla.html` is the original single-file UI and still works  
> without any build step if you open it directly in the browser.

---

## ğŸ”Œ API Reference

### `GET /`
Health check â€” returns model status and best validation accuracy.

```json
{
  "status": "TamilVision API Online",
  "accuracy": "97.43%",
  "device": "cuda",
  "classes": 156
}
```

### `POST /predict`
Accepts a Tamil character image as `multipart/form-data`.

| Field | Type | Description |
|---|---|---|
| `file` | `UploadFile` | PNG, JPG, or BMP image of a Tamil character |

**Response**
```json
{
  "predictions": [
    { "predicted_character": "à®•",  "confidence": 0.973214, "label_id": 36 },
    { "predicted_character": "à®•à®¿", "confidence": 0.018432, "label_id": 59 },
    { "predicted_character": "à®•à¯€", "confidence": 0.005102, "label_id": 82 }
  ]
}
```

---

## ğŸ“š Dataset

This project uses the **uTHCD** (University of Tamil Nadu Handwritten Character Dataset).

1. Download from [Mendeley Data](https://data.mendeley.com/datasets/p36fh3jgbm/1)
2. Extract to `data/raw/`
3. Expected layout:

```
data/raw/uTHCD_b(80-20-split)/80-20-split/train-test-classwise/
    train/
        à®…/   0001_0.bmp  ...
        à®†/   ...
    test/
        ...
```

---

## ğŸ‹ï¸ Training Your Own Model

```bash
# Edit the data split path in src/train.py if needed, then:
python src/train.py
```

The best checkpoint is saved automatically to `models/best_model.pth` whenever validation Top-1 accuracy improves.

---

## ğŸ¤ Contributing

Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.

---

## ğŸ“ License

MIT License â€” see [LICENSE](LICENSE) for details.

---

<p align="center">Built with â¤ï¸ for Tamil Language Preservation</p>
